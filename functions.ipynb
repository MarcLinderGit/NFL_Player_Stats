{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install -v pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"https://www.nfl.com\"\n",
    "current_season  = \"2023\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Player Stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define function to get links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function retrieves the links for player or team statistics based on the specified level (either \"player\" or \"team\") and season. It scrapes the NFL website for the relevant links."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to get statistics links\n",
    "def get_links(level, season, base_url=\"https://www.nfl.com\"):\n",
    "    \"\"\"\n",
    "    Retrieves links for player or team statistics based on the specified level and season.\n",
    "    \n",
    "    Args:\n",
    "        level (str): The statistics level, either \"player\" or \"team\".\n",
    "        season (str): The NFL season for which statistics are retrieved.\n",
    "        base_url (str, optional): The base URL for the NFL website. Defaults to \"https://www.nfl.com\".\n",
    "        \n",
    "    Returns:\n",
    "        list: A list of links to statistics pages.\n",
    "    \"\"\"\n",
    "    # Initialize a list to store the links\n",
    "    all_links = []\n",
    "\n",
    "    if level == \"player\":\n",
    "        # Request the raw HTML for player statistics page\n",
    "        html = requests.get(\"https://www.nfl.com/stats/player-stats/\")\n",
    "        # Parse the HTML content using BeautifulSoup\n",
    "        soup = BeautifulSoup(html.content, 'html.parser')\n",
    "        # Find all list items with class 'd3-o-tabs__list-item'\n",
    "        li_elements = soup.find_all('li', class_='d3-o-tabs__list-item')\n",
    "    elif level == \"team\":\n",
    "        # Request the raw HTML for team statistics page\n",
    "        html = requests.get(\"https://www.nfl.com/stats/team-stats/\")\n",
    "        # Parse the HTML content using BeautifulSoup\n",
    "        soup = BeautifulSoup(html.content, 'html.parser')\n",
    "        # Find the unordered list element with class 'd3-o-tabbed-controls-selector__list'\n",
    "        ul_element = soup.find('ul', class_='d3-o-tabbed-controls-selector__list')\n",
    "        # Find all list items within the unordered list\n",
    "        li_elements = ul_element.find_all('li')\n",
    "\n",
    "    # Initialize a list to store the href values\n",
    "    href_values = []\n",
    "\n",
    "    # Iterate through the list items and extract href values from anchor tags\n",
    "    for li in li_elements:\n",
    "        a_tag = li.find('a')\n",
    "        if a_tag:\n",
    "            href = a_tag['href']\n",
    "            href_values.append(href)\n",
    "\n",
    "    # Loop through href_values and fetch links for each URL\n",
    "    for href in href_values:\n",
    "        url = base_url + href\n",
    "        html = requests.get(url)\n",
    "        soup = BeautifulSoup(html.content, \"html.parser\")\n",
    "        # Find all list items with class 'd3-o-tabs__list-item'\n",
    "        a_elements = soup.find_all('li', class_='d3-o-tabs__list-item')\n",
    "        # Extract and store links from anchor tags\n",
    "        links = [base_url + element.find('a')['href'] for element in a_elements]\n",
    "        all_links.extend(links)  # Append the links to the all_links list\n",
    "    \n",
    "    # Replace \"2023\" with the value of the season variable we want to scrape\n",
    "    all_links = [link.replace('2023', season) for link in all_links]\n",
    "    return all_links\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function collects sub-pages for different statistics categories based on the provided unit_links. It navigates through pagination to gather all relevant pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to get sub-pages for different categories\n",
    "def get_sub_pages(unit_links):\n",
    "    \"\"\"\n",
    "    Collects sub-pages for different statistics categories based on provided unit_links.\n",
    "    \n",
    "    Args:\n",
    "        unit_links (dict): A dictionary of unit-specific statistics links.\n",
    "        \n",
    "    Returns:\n",
    "        dict: A dictionary of sub-pages for each unit and category.\n",
    "    \"\"\"\n",
    "    # Create a dictionary to store the category names and their corresponding page links\n",
    "    sub_pages = {}\n",
    "    base_url = \"https://www.nfl.com\"  # Assuming you have a base URL\n",
    "\n",
    "    for unit, category_links in unit_links.items():\n",
    "        sub_pages[unit] = {}\n",
    "\n",
    "        for category, link in category_links.items():\n",
    "            page_count = 0  # Initialize page count\n",
    "            current_link = link  # Use the provided link as the starting point\n",
    "            current_stat = category  # Set the current_stat to the category name\n",
    "\n",
    "            # Initialize the category's dictionary\n",
    "            sub_pages[unit][current_stat] = {page_count: current_link}\n",
    "\n",
    "            # Create an infinite loop to scrape data from multiple pages\n",
    "            while True:\n",
    "                # Request raw HTML for the current page\n",
    "                response = requests.get(current_link)\n",
    "\n",
    "                # Check if the request was successful\n",
    "                if response.status_code == 200:\n",
    "                    # Create a BeautifulSoup object to parse the HTML\n",
    "                    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "                    # Find the \"Next Page\" link\n",
    "                    next_page_link = soup.find('a', class_='nfl-o-table-pagination__next')\n",
    "\n",
    "                    if next_page_link:\n",
    "                        # Extract the 'href' attribute\n",
    "                        href = next_page_link['href']\n",
    "\n",
    "                        # Update current_link with the next page's URL\n",
    "                        current_link = base_url + href\n",
    "                        page_count += 1  # Increment page count\n",
    "\n",
    "                        # Add the link to the category's dictionary\n",
    "                        sub_pages[unit][current_stat][page_count] = current_link\n",
    "                    else:\n",
    "                        print(f\"No more pages to scrape for {unit} - {current_stat}.\")\n",
    "                        break  # Exit the loop when there are no more pages\n",
    "                else:\n",
    "                    print(f\"Error: Unable to fetch data from {current_link} for {unit} - {current_stat}.\")\n",
    "                    break  # Exit the loop on request error\n",
    "\n",
    "    # Display the collected category pages and their links\n",
    "    for unit, categories in sub_pages.items():\n",
    "        for category, pages in categories.items():\n",
    "            print(f\"{unit} - {category} Sub-Pages:\")\n",
    "            for page_num, page_link in pages.items():\n",
    "                print(f\"Page {page_num}: {page_link}\")\n",
    "    return sub_pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Function to format links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The format_links function organizes and formats the links for player or team statistics into a dictionary structure, making it easier to access specific categories and sub-pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to format and organize links\n",
    "def format_links(level, season):\n",
    "    \"\"\"\n",
    "    Formats and organizes statistics links into a dictionary structure for easy access.\n",
    "    \n",
    "    Args:\n",
    "        level (str): The statistics level, either \"player\" or \"team\".\n",
    "        season (str): The NFL season for which statistics are retrieved.\n",
    "        \n",
    "    Returns:\n",
    "        dict: A dictionary of formatted statistics links.\n",
    "    \"\"\"\n",
    "    # Create a dictionary to store the links for each unit and its categories\n",
    "    unit_links = {}\n",
    "\n",
    "    if level == \"player\":\n",
    "        # Define the URL for the player level\n",
    "        all_links = get_links(level, season, base_url)  # Pass the URL as an argument\n",
    "        # Create a dictionary to store the links\n",
    "        team_stats_dict = {}\n",
    "\n",
    "        for link in all_links:\n",
    "            # Split the link by \"/\"\n",
    "            parts = link.split('/')\n",
    "            # Get the keys and values\n",
    "            unit = \"individual\" # e.g., individual, offense, defense, special-teams\n",
    "            category = parts[6]\n",
    "            url = link\n",
    "            # Add to the dictionary\n",
    "            if unit not in team_stats_dict:\n",
    "                team_stats_dict[unit] = {}\n",
    "            team_stats_dict[unit][category] = url\n",
    "\n",
    "    if level == \"team\":\n",
    "        all_links = get_links(level, season, base_url)\n",
    "\n",
    "        # Create a dictionary to store the links\n",
    "        team_stats_dict = {}\n",
    "\n",
    "        for link in all_links:\n",
    "            # Split the link by \"/\"\n",
    "            parts = link.split('/')\n",
    "            # Get the keys and values\n",
    "            unit = parts[5] # e.g., offense, defense, special-teams\n",
    "            category = parts[6] # e.g., passing, rushing etc.\n",
    "            url = link\n",
    "            # Add to the dictionary\n",
    "            if unit not in team_stats_dict:\n",
    "                team_stats_dict[unit] = {}\n",
    "            team_stats_dict[unit][category] = url\n",
    "\n",
    "    # Get stat category names and update unit_links\n",
    "    stat_cols = {}\n",
    "\n",
    "    for outer_key, inner_dict in team_stats_dict.items():\n",
    "        inner_keys = list(inner_dict.keys())\n",
    "        \n",
    "        if outer_key in stat_cols:\n",
    "            stat_cols[outer_key].extend(inner_keys)\n",
    "        else:\n",
    "            stat_cols[outer_key] = inner_keys\n",
    "\n",
    "    # Update unit_links with the fetched links\n",
    "    unit_links = team_stats_dict\n",
    "    unit_links = get_sub_pages(unit_links)\n",
    "    return unit_links\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Function to Check/Create Directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function checks if a directory exists at the specified path and creates it if it doesn't. It's used to ensure data storage directories are in place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to create a directory if it doesn't exist\n",
    "def create_directory_if_not_exists(directory_path):\n",
    "    \"\"\"\n",
    "    Checks if a directory exists at the specified path and creates it if it doesn't.\n",
    "\n",
    "    Args:\n",
    "        directory_path (str): The path of the directory to be checked/created.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(directory_path):\n",
    "        try:\n",
    "            os.makedirs(directory_path)\n",
    "            print(f'Directory \"{directory_path}\" has been created.')\n",
    "        except OSError as e:\n",
    "            print(f'Error: Failed to create directory \"{directory_path}\".')\n",
    "            print(e)\n",
    "    else:\n",
    "        print(f'Directory \"{directory_path}\" already exists.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Function to Scrape and Process Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scrape_and_process_data function is responsible for scraping data for a specific category, processing it, and storing it in the appropriate directory. It iterates through sub-pages to gather data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to scrape and process data\n",
    "def scrape_and_process_data(unit, category, level, unit_directory_path, unit_links):\n",
    "    \"\"\"\n",
    "    Scrapes data for a specific category, processes it, and stores it in the appropriate directory.\n",
    "\n",
    "    Args:\n",
    "        unit (str): The unit (e.g., offense, defense) for which data is scraped.\n",
    "        category (str): The specific statistics category (e.g., passing, rushing).\n",
    "        level (str): The statistics level, either \"player\" or \"team\".\n",
    "        unit_directory_path (str): The path of the directory where data is stored.\n",
    "        unit_links (dict): A dictionary of unit-specific statistics links.\n",
    "    \"\"\"\n",
    "    # Create a list to store DataFrames for the current category\n",
    "    category_dfs = []\n",
    "\n",
    "    # Loop through the sub-pages for the current category\n",
    "    for page_num, page_url in unit_links[unit][category].items():\n",
    "        # Request raw HTML for the current page\n",
    "        response = requests.get(page_url)\n",
    "\n",
    "        # Check if the request was successful\n",
    "        if response.status_code == 200:\n",
    "            # Create a BeautifulSoup object to parse the HTML\n",
    "            soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "            # Find all elements with the class 'd3-o-player-stats--detailed'\n",
    "            stats = soup.find_all(attrs={\"class\": f'd3-o-{level}-stats--detailed'})\n",
    "\n",
    "            # Initialize lists to collect data\n",
    "            stat_val = []\n",
    "            stat_col = []\n",
    "\n",
    "            # Loop through each <tr> element to extract and collect the text from <td> elements\n",
    "            for row in stats:\n",
    "                # This gets the stat names\n",
    "                header_cells = row.find_all('th')\n",
    "\n",
    "                if len(header_cells) > 0:\n",
    "                    for cell in header_cells:\n",
    "                        stat_col.append(cell.get_text(strip=True))\n",
    "\n",
    "                # This gets the stats\n",
    "                data_cells = row.find_all('td')\n",
    "\n",
    "                if len(data_cells) > 0:\n",
    "                    for cell in data_cells:\n",
    "                        stat_val.append(cell.get_text(strip=True))\n",
    "\n",
    "            # Determine the number of columns in each row\n",
    "            num_columns = len(stat_col) if stat_col else 1  # Use 1 if stat_col is empty\n",
    "\n",
    "            # Split the list into rows\n",
    "            rows = [stat_val[i:i + num_columns] for i in range(0, len(stat_val), num_columns)]\n",
    "\n",
    "            # Create a DataFrame for the current category and page\n",
    "            df = pd.DataFrame(rows, columns=stat_col)\n",
    "\n",
    "            # Append the DataFrame to the list for the current category\n",
    "            category_dfs.append(df)\n",
    "        else:\n",
    "            print(f\"Error: Unable to fetch data from {page_url} for {category}.\")\n",
    "\n",
    "    # Concatenate dataframes for the current category into one\n",
    "    if category_dfs:\n",
    "        merged_df = pd.concat(category_dfs, ignore_index=True)\n",
    "\n",
    "        # Check if the DataFrame has a \"Team\" column before attempting to remove the duplicated part\n",
    "        if 'Team' in merged_df.columns:\n",
    "            # Remove duplicated part from the \"Team\" column\n",
    "            merged_df['Team'] = merged_df['Team'].apply(lambda x: x[:len(x) // 2])\n",
    "\n",
    "        # Create the directory if it doesn't exist\n",
    "        create_directory_if_not_exists(unit_directory_path)\n",
    "\n",
    "        # Specify the file path within the unit's directory\n",
    "        csv_file_path = os.path.join(unit_directory_path, category + '.csv')\n",
    "\n",
    "        # Export the DataFrame to a CSV file\n",
    "        merged_df.to_csv(csv_file_path, index=False)  # Set index=False to exclude the index column\n",
    "\n",
    "        print(f'DataFrame for category \"{category}\" in unit \"{unit}\" has been exported to {csv_file_path}')\n",
    "    else:\n",
    "        print(f\"No data found for category '{category}' in unit '{unit}'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customer Year Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValidSeasonError(Exception):\n",
    "   def __init__(self, season):\n",
    "       self.season = season\n",
    "       \n",
    "   def __str__(self):\n",
    "        return 'The ' + str(self.season) + ' Season is not within the database ranging from 1970 to current season. ' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Function to  Initiate Scraping Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function serves as the entry point for collecting statistics data. It specifies the current season and week, organizes data storage directories, and initiates the scraping process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stats(level, season):\n",
    "    \"\"\"\n",
    "    Initiates the data scraping process for player or team statistics.\n",
    "\n",
    "    Args:\n",
    "        level (str): The statistics level, either \"player\" or \"team\".\n",
    "        season (str): The NFL season for which statistics are retrieved.\n",
    "    \"\"\"\n",
    "    # Check if the season is in string format    \n",
    "    if isinstance(season, str):\n",
    "        try:\n",
    "            season = int(season)\n",
    "            print(\"Converted season to integer.\")\n",
    "        except ValueError:\n",
    "            print('The season cannot be converted to integer.')\n",
    "            return\n",
    "    # Convert to string    \n",
    "    else:\n",
    "        season = str(season)\n",
    "\n",
    "    if int(season) < 1970:\n",
    "        raise ValidSeasonError(season)\n",
    "    else:\n",
    "        # Combine the base directory path with the current week for the current season,\n",
    "        # else store data in 'reg' (regular season) directory\n",
    "        global current_season, current_week\n",
    "        directory_path = os.path.join('data', season, level)\n",
    "        if int(season) == current_season:\n",
    "            directory_path = os.path.join(directory_path, f'week{current_week}')\n",
    "\n",
    "        unit_links = format_links(level, season)\n",
    "\n",
    "        if level == \"team\":\n",
    "            for unit, categories in unit_links.items():\n",
    "                for category, _ in categories.items():\n",
    "                    # Create a subdirectory for the current unit\n",
    "                    unit_directory_path = os.path.join(directory_path, unit)\n",
    "\n",
    "                    # Call scrape_and_process_data with unit and category\n",
    "                    scrape_and_process_data(unit, category, level, unit_directory_path, unit_links)\n",
    "\n",
    "        elif level == \"player\":\n",
    "            for unit, categories in unit_links.items():\n",
    "                for category, _ in categories.items():\n",
    "                    # Directly use the week1 directory for player-level data\n",
    "                    unit_directory_path = directory_path\n",
    "\n",
    "                    # Pass unit and category to the scrape_and_process_data function\n",
    "                    scrape_and_process_data(unit, category, level, unit_directory_path, unit_links)\n",
    "        else:\n",
    "            print(\"Invalid level specified.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
