{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install lxml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Base Url\n",
    "base_url = \"https://www.nfl.com\"\n",
    "\n",
    "# Define Week\n",
    "current_week = 1\n",
    "\n",
    "# Define level we want stats for\n",
    "level = \"player\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Player Stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Pages to All Tabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Category                                               Link\n",
      "0           Passing  https://www.nfl.com/stats/player-stats/categor...\n",
      "1           Rushing  https://www.nfl.com/stats/player-stats/categor...\n",
      "2         Receiving  https://www.nfl.com/stats/player-stats/categor...\n",
      "3           Fumbles  https://www.nfl.com/stats/player-stats/categor...\n",
      "4           Tackles  https://www.nfl.com/stats/player-stats/categor...\n",
      "5     Interceptions  https://www.nfl.com/stats/player-stats/categor...\n",
      "6       Field Goals  https://www.nfl.com/stats/player-stats/categor...\n",
      "7          Kickoffs  https://www.nfl.com/stats/player-stats/categor...\n",
      "8   Kickoff Returns  https://www.nfl.com/stats/player-stats/categor...\n",
      "9           Punting  https://www.nfl.com/stats/player-stats/categor...\n",
      "10     Punt Returns  https://www.nfl.com/stats/player-stats/categor...\n"
     ]
    }
   ],
   "source": [
    "# Request raw HTML\n",
    "html = requests.get(\"https://www.nfl.com/stats/player-stats\")\n",
    "\n",
    "# Create a BeautifulSoup object called `soup` to traverse the combined HTML\n",
    "soup = BeautifulSoup(html.content, \"html.parser\")\n",
    "\n",
    "# Find all <a> elements within <li> elements\n",
    "a_elements = soup.find_all('li', class_='d3-o-tabs__list-item')\n",
    "\n",
    "# Extract the text from the <a> elements\n",
    "tabs = [element.find('a').get_text() for element in a_elements]\n",
    "\n",
    "# Extract the links (href attributes) from the <a> elements\n",
    "links = [base_url + element.find('a')['href'] for element in a_elements]\n",
    "\n",
    "# Create a dictionary to store the category names and their corresponding links\n",
    "stat_pages = dict(zip(tabs, links))\n",
    "\n",
    "# Print the dictionary as a table-like format using Pandas\n",
    "stat_pages_df = pd.DataFrame(list(stat_pages.items()), columns=['Category', 'Link'])\n",
    "\n",
    "print(stat_pages_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary to store the category names and their corresponding page links\n",
    "category_pages = {}\n",
    "\n",
    "# Loop through each category's link and scrape data\n",
    "for index, row in stat_pages_df.iterrows():\n",
    "    current_link = row['Link']\n",
    "    current_stat = row['Category']\n",
    "    page_count = 0  # Initialize page count\n",
    "    category_pages[current_stat] = {page_count: current_link}  # Initialize the category's dictionary\n",
    "    while current_link:\n",
    "        # Request raw HTML for the current page\n",
    "        response = requests.get(current_link)\n",
    "        \n",
    "        # Check if the request was successful\n",
    "        if response.status_code == 200:\n",
    "            # Create a BeautifulSoup object to parse the HTML\n",
    "            soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "            # Scrape the data from the current page here\n",
    "            # (You can add your scraping logic here)\n",
    "            # Example: print(soup.title.text) to print the page title\n",
    "            \n",
    "            # Find the \"Next Page\" link\n",
    "            next_page_link = soup.find('a', class_='nfl-o-table-pagination__next')\n",
    "            \n",
    "            if next_page_link:\n",
    "                # Extract the 'href' attribute\n",
    "                href = next_page_link['href']\n",
    "\n",
    "                # Update current_link with the next page's URL\n",
    "                current_link = base_url + href\n",
    "                page_count += 1  # Increment page count\n",
    "                category_pages[current_stat][page_count] = current_link  # Add the link to the category's dictionary\n",
    "            else:\n",
    "                # No more pages to scrape, exit the loop\n",
    "                break\n",
    "        else:\n",
    "            print(\"Error: Unable to fetch the page.\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Export DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory \"data\\player\\week1\" has been created.\n",
      "DataFrame for category \"Passing\" has been exported to data\\player\\week1\\Passing.csv\n",
      "DataFrame for category \"Rushing\" has been exported to data\\player\\week1\\Rushing.csv\n",
      "DataFrame for category \"Receiving\" has been exported to data\\player\\week1\\Receiving.csv\n",
      "DataFrame for category \"Fumbles\" has been exported to data\\player\\week1\\Fumbles.csv\n",
      "DataFrame for category \"Tackles\" has been exported to data\\player\\week1\\Tackles.csv\n",
      "DataFrame for category \"Interceptions\" has been exported to data\\player\\week1\\Interceptions.csv\n",
      "DataFrame for category \"Field Goals\" has been exported to data\\player\\week1\\Field Goals.csv\n",
      "DataFrame for category \"Kickoffs\" has been exported to data\\player\\week1\\Kickoffs.csv\n",
      "DataFrame for category \"Kickoff Returns\" has been exported to data\\player\\week1\\Kickoff Returns.csv\n",
      "DataFrame for category \"Punting\" has been exported to data\\player\\week1\\Punting.csv\n",
      "DataFrame for category \"Punt Returns\" has been exported to data\\player\\week1\\Punt Returns.csv\n"
     ]
    }
   ],
   "source": [
    "# Combine the base directory path with the current week\n",
    "directory_path = os.path.join('data', level, 'week' + str(current_week))\n",
    "\n",
    "# Check if the directory exists\n",
    "if not os.path.exists(directory_path):\n",
    "    # Create the directory if it doesn't exist\n",
    "    os.makedirs(directory_path)\n",
    "    print(f'Directory \"{directory_path}\" has been created.')\n",
    "else:\n",
    "    print(f'Directory \"{directory_path}\" already exists.')\n",
    "\n",
    "# Loop through each category in category_pages\n",
    "for category, links in category_pages.items():\n",
    "    # Initialize a list to store DataFrames\n",
    "    dataframes = []\n",
    "\n",
    "    # Loop through the links for the current category\n",
    "    for page, link in enumerate(links):\n",
    "        # Request raw HTML for the current page\n",
    "        response = requests.get(links[link])\n",
    "\n",
    "        # Check if the request was successful\n",
    "        if response.status_code == 200:\n",
    "            # Create a BeautifulSoup object to parse the HTML\n",
    "            soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "            # Find all elements with the class 'd3-o-player-stats--detailed'\n",
    "            stats = soup.find_all(attrs={\"class\": 'd3-o-' + level + '-stats--detailed'})\n",
    "\n",
    "            # Initialize lists to collect data\n",
    "            stat_val = []\n",
    "            stat_col = []\n",
    "\n",
    "            # Loop through each <tr> element to extract and collect the text from <td> elements\n",
    "            for row in stats:\n",
    "                # This gets the stat names\n",
    "                header_cells = row.find_all('th')\n",
    "\n",
    "                if len(header_cells) > 0:\n",
    "                    for cell in header_cells:\n",
    "                        stat_col.append(cell.get_text(strip=True))\n",
    "\n",
    "                # This gets the stats\n",
    "                data_cells = row.find_all('td')\n",
    "\n",
    "                if len(data_cells) > 0:\n",
    "                    for cell in data_cells:\n",
    "                        stat_val.append(cell.get_text(strip=True))\n",
    "\n",
    "            # Determine the number of columns in each row\n",
    "            num_columns = len(stat_col) if stat_col else 1  # Use 1 if stat_col is empty\n",
    "\n",
    "            # Split the list into rows\n",
    "            rows = [stat_val[i:i + num_columns] for i in range(0, len(stat_val), num_columns)]\n",
    "\n",
    "            # Create a DataFrame for the current category and page\n",
    "            df = pd.DataFrame(rows, columns=stat_col)\n",
    "\n",
    "            # Convert all columns except \"Player\" to numeric\n",
    "            numeric_columns = df.columns.difference(['Player'])\n",
    "            df[numeric_columns] = df[numeric_columns].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "            # Add the DataFrame to the list\n",
    "            dataframes.append(df)\n",
    "        else:\n",
    "            print(\"Error: Unable to fetch the page.\")\n",
    "            break\n",
    "\n",
    "    # Combine all DataFrames into one final DataFrame\n",
    "    final_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "    # Specify the file path where you want to save the CSV file\n",
    "    csv_file_path = os.path.join(directory_path, category + '.csv')\n",
    "\n",
    "    # Export the DataFrame to a CSV file\n",
    "    final_df.to_csv(csv_file_path, index=False)  # Set index=False to exclude the index column\n",
    "\n",
    "    print(f'DataFrame for category \"{category}\" has been exported to {csv_file_path}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Team Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "base_url = \"https://www.nfl.com\"\n",
    "current_week = 1  # Replace with the current week number\n",
    "\n",
    "# Define level we want stats for\n",
    "level = \"player\"  # Replace with \"team\" or \"player\" as needed\n",
    "\n",
    "# Define unit\n",
    "lst = [\"offense\", \"defense\", \"special-teams\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Offense': {'Passing': 'https://www.nfl.com/stats/team-stats/offense/passing/2023/reg/all', 'Rushing': 'https://www.nfl.com/stats/team-stats/offense/rushing/2023/reg/all', 'Receiving': 'https://www.nfl.com/stats/team-stats/offense/receiving/2023/reg/all', 'Scoring': 'https://www.nfl.com/stats/team-stats/offense/scoring/2023/reg/all', 'Downs': 'https://www.nfl.com/stats/team-stats/offense/downs/2023/reg/all'}, 'Defense': {'Passing': 'https://www.nfl.com/stats/team-stats/defense/passing/2023/reg/all', 'Rushing': 'https://www.nfl.com/stats/team-stats/defense/rushing/2023/reg/all', 'Receiving': 'https://www.nfl.com/stats/team-stats/defense/receiving/2023/reg/all', 'Scoring': 'https://www.nfl.com/stats/team-stats/defense/scoring/2023/reg/all', 'Tackles': 'https://www.nfl.com/stats/team-stats/defense/tackles/2023/reg/all', 'Downs': 'https://www.nfl.com/stats/team-stats/defense/downs/2023/reg/all', 'Fumbles': 'https://www.nfl.com/stats/team-stats/defense/fumbles/2023/reg/all', 'Interceptions': 'https://www.nfl.com/stats/team-stats/defense/interceptions/2023/reg/all'}, 'Special-teams': {'Field Goals': 'https://www.nfl.com/stats/team-stats/special-teams/field-goals/2023/reg/all', 'Scoring': 'https://www.nfl.com/stats/team-stats/special-teams/scoring/2023/reg/all', 'Kickoffs': 'https://www.nfl.com/stats/team-stats/special-teams/kickoffs/2023/reg/all', 'Kickoff Returns': 'https://www.nfl.com/stats/team-stats/special-teams/kickoff-returns/2023/reg/all', 'Punting': 'https://www.nfl.com/stats/team-stats/special-teams/punts/2023/reg/all', 'Punt Returns': 'https://www.nfl.com/stats/team-stats/special-teams/punt-returns/2023/reg/all'}}\n"
     ]
    }
   ],
   "source": [
    "# Create a dictionary to store the links for each unit and its categories\n",
    "unit_links = {}\n",
    "\n",
    "for unit in lst:\n",
    "    # Request raw HTML\n",
    "    html = requests.get(base_url + \"/stats/team-stats/\" + unit + \"/passing/2023/reg/all\")\n",
    "\n",
    "    # Create a BeautifulSoup object called `soup` to traverse the combined HTML\n",
    "    soup = BeautifulSoup(html.content, \"html.parser\")\n",
    "\n",
    "    # Find all <a> elements within <li> elements\n",
    "    a_elements = soup.find_all('li', class_='d3-o-tabs__list-item')\n",
    "\n",
    "    # Extract the text from the <a> elements\n",
    "    tabs = [element.find('a').get_text() for element in a_elements]\n",
    "\n",
    "    # Extract the links (href attributes) from the <a> elements\n",
    "    links = [base_url + element.find('a')['href'] for element in a_elements]\n",
    "\n",
    "    # Create a dictionary to store the category names and their corresponding links\n",
    "    category_links = dict(zip(tabs, links))\n",
    "\n",
    "    # Add the category links to the unit_links dictionary\n",
    "    unit_links[unit.capitalize()] = category_links\n",
    "\n",
    "# Print the nested dictionary\n",
    "print(unit_links)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Category                                               Link\n",
      "0           Passing  https://www.nfl.com/stats/player-stats/categor...\n",
      "1           Rushing  https://www.nfl.com/stats/player-stats/categor...\n",
      "2         Receiving  https://www.nfl.com/stats/player-stats/categor...\n",
      "3           Fumbles  https://www.nfl.com/stats/player-stats/categor...\n",
      "4           Tackles  https://www.nfl.com/stats/player-stats/categor...\n",
      "5     Interceptions  https://www.nfl.com/stats/player-stats/categor...\n",
      "6       Field Goals  https://www.nfl.com/stats/player-stats/categor...\n",
      "7          Kickoffs  https://www.nfl.com/stats/player-stats/categor...\n",
      "8   Kickoff Returns  https://www.nfl.com/stats/player-stats/categor...\n",
      "9           Punting  https://www.nfl.com/stats/player-stats/categor...\n",
      "10     Punt Returns  https://www.nfl.com/stats/player-stats/categor...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Create a dictionary to store the links for each unit and its categories\n",
    "unit_links = {}\n",
    "\n",
    "# Function to process HTML content and return category links\n",
    "def process_html(url):\n",
    "    # Request raw HTML\n",
    "    html = requests.get(url)\n",
    "\n",
    "    # Create a BeautifulSoup object called `soup` to traverse the combined HTML\n",
    "    soup = BeautifulSoup(html.content, \"html.parser\")\n",
    "\n",
    "    # Find all <a> elements within <li> elements\n",
    "    a_elements = soup.find_all('li', class_='d3-o-tabs__list-item')\n",
    "\n",
    "    # Extract the text from the <a> elements\n",
    "    tabs = [element.find('a').get_text() for element in a_elements]\n",
    "\n",
    "    # Extract the links (href attributes) from the <a> elements\n",
    "    links = [base_url + element.find('a')['href'] for element in a_elements]\n",
    "\n",
    "    # Create a dictionary to store the category names and their corresponding links\n",
    "    category_links = dict(zip(tabs, links))\n",
    "\n",
    "    return category_links\n",
    "\n",
    "# Define the URL to scrape based on the specified level\n",
    "url = None  # Initialize URL to None\n",
    "\n",
    "if level == \"player\":\n",
    "    url = f\"{base_url}/stats/player-stats\"\n",
    "elif level == \"team\":\n",
    "    lst = [\"offense\", \"defense\", \"special-teams\"]\n",
    "else:\n",
    "    print(\"Invalid level specified.\")\n",
    "\n",
    "# Process the URL and build the unit_links dictionary\n",
    "if url:\n",
    "    unit_links = process_html(url)\n",
    "    \n",
    "    # Print the dictionary as a table-like format using Pandas\n",
    "    stat_pages_df = pd.DataFrame(list(unit_links.items()), columns=['Category', 'Link'])\n",
    "    print(stat_pages_df)\n",
    "\n",
    "if level == \"team\":\n",
    "    for unit in lst:\n",
    "        # Define the URL for the current unit\n",
    "        url = f\"{base_url}/stats/team-stats/{unit}/passing/2023/reg/all\"\n",
    "        category_links = process_html(url)\n",
    "\n",
    "        # Add the category links to the unit_links dictionary\n",
    "        unit_links[unit.capitalize()] = category_links\n",
    "\n",
    "    # Print the nested dictionary\n",
    "    print(unit_links)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory \"data\\team\\week1\" has been created.\n",
      "DataFrame for category \"Passing\" in unit \"Offense\" has been exported to data\\team\\week1\\Offense\\Passing.csv\n",
      "DataFrame for category \"Rushing\" in unit \"Offense\" has been exported to data\\team\\week1\\Offense\\Rushing.csv\n",
      "DataFrame for category \"Receiving\" in unit \"Offense\" has been exported to data\\team\\week1\\Offense\\Receiving.csv\n",
      "DataFrame for category \"Scoring\" in unit \"Offense\" has been exported to data\\team\\week1\\Offense\\Scoring.csv\n",
      "DataFrame for category \"Downs\" in unit \"Offense\" has been exported to data\\team\\week1\\Offense\\Downs.csv\n",
      "DataFrame for category \"Passing\" in unit \"Defense\" has been exported to data\\team\\week1\\Defense\\Passing.csv\n",
      "DataFrame for category \"Rushing\" in unit \"Defense\" has been exported to data\\team\\week1\\Defense\\Rushing.csv\n",
      "DataFrame for category \"Receiving\" in unit \"Defense\" has been exported to data\\team\\week1\\Defense\\Receiving.csv\n",
      "DataFrame for category \"Scoring\" in unit \"Defense\" has been exported to data\\team\\week1\\Defense\\Scoring.csv\n",
      "DataFrame for category \"Tackles\" in unit \"Defense\" has been exported to data\\team\\week1\\Defense\\Tackles.csv\n",
      "DataFrame for category \"Downs\" in unit \"Defense\" has been exported to data\\team\\week1\\Defense\\Downs.csv\n",
      "DataFrame for category \"Fumbles\" in unit \"Defense\" has been exported to data\\team\\week1\\Defense\\Fumbles.csv\n",
      "DataFrame for category \"Interceptions\" in unit \"Defense\" has been exported to data\\team\\week1\\Defense\\Interceptions.csv\n",
      "DataFrame for category \"Field Goals\" in unit \"Special-teams\" has been exported to data\\team\\week1\\Special-teams\\Field Goals.csv\n",
      "DataFrame for category \"Scoring\" in unit \"Special-teams\" has been exported to data\\team\\week1\\Special-teams\\Scoring.csv\n",
      "DataFrame for category \"Kickoffs\" in unit \"Special-teams\" has been exported to data\\team\\week1\\Special-teams\\Kickoffs.csv\n",
      "DataFrame for category \"Kickoff Returns\" in unit \"Special-teams\" has been exported to data\\team\\week1\\Special-teams\\Kickoff Returns.csv\n",
      "DataFrame for category \"Punting\" in unit \"Special-teams\" has been exported to data\\team\\week1\\Special-teams\\Punting.csv\n",
      "DataFrame for category \"Punt Returns\" in unit \"Special-teams\" has been exported to data\\team\\week1\\Special-teams\\Punt Returns.csv\n"
     ]
    }
   ],
   "source": [
    "# Combine the base directory path with the current week\n",
    "directory_path = os.path.join('data', level, 'week' + str(current_week))\n",
    "\n",
    "# Check if the directory exists\n",
    "if not os.path.exists(directory_path):\n",
    "    # Create the directory if it doesn't exist\n",
    "    os.makedirs(directory_path)\n",
    "    print(f'Directory \"{directory_path}\" has been created.')\n",
    "else:\n",
    "    print(f'Directory \"{directory_path}\" already exists.')\n",
    "\n",
    "# Loop through each unit in unit_links\n",
    "for unit, categories in unit_links.items():\n",
    "    # Create a subdirectory for the current unit\n",
    "    unit_directory_path = os.path.join(directory_path, unit)\n",
    "    \n",
    "    if not os.path.exists(unit_directory_path):\n",
    "        os.makedirs(unit_directory_path)\n",
    "\n",
    "    # Loop through the categories for the current unit\n",
    "    for category, link in categories.items():\n",
    "        # Request raw HTML for the current page\n",
    "        response = requests.get(link)\n",
    "\n",
    "        # Check if the request was successful\n",
    "        if response.status_code == 200:\n",
    "            # Create a BeautifulSoup object to parse the HTML\n",
    "            soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "            # Find all elements with the class 'd3-o-player-stats--detailed'\n",
    "            stats = soup.find_all(attrs={\"class\": 'd3-o-' + level + '-stats--detailed'})\n",
    "\n",
    "            # Initialize lists to collect data\n",
    "            stat_val = []\n",
    "            stat_col = []\n",
    "\n",
    "            # Loop through each <tr> element to extract and collect the text from <td> elements\n",
    "            for row in stats:\n",
    "                # This gets the stat names\n",
    "                header_cells = row.find_all('th')\n",
    "\n",
    "                if len(header_cells) > 0:\n",
    "                    for cell in header_cells:\n",
    "                        stat_col.append(cell.get_text(strip=True))\n",
    "\n",
    "                # This gets the stats\n",
    "                data_cells = row.find_all('td')\n",
    "\n",
    "                if len(data_cells) > 0:\n",
    "                    for cell in data_cells:\n",
    "                        stat_val.append(cell.get_text(strip=True))\n",
    "\n",
    "            # Determine the number of columns in each row\n",
    "            num_columns = len(stat_col) if stat_col else 1  # Use 1 if stat_col is empty\n",
    "            \n",
    "            # Split the list into rows\n",
    "            rows = [stat_val[i:i + num_columns] for i in range(0, len(stat_val), num_columns)]\n",
    "\n",
    "            # Create a DataFrame for the current category\n",
    "            df = pd.DataFrame(rows, columns=stat_col)\n",
    "\n",
    "            # Convert all columns except \"Team\" to numeric\n",
    "            numeric_columns = df.columns.difference(['Team'])\n",
    "            df[numeric_columns] = df[numeric_columns].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "            # Check if the DataFrame has a \"Team\" column before attempting to remove the duplicated part\n",
    "            if 'Team' in df.columns:\n",
    "                # Remove duplicated part from the \"Team\" column\n",
    "                df['Team'] = df['Team'].apply(lambda x: x[:len(x)//2])\n",
    "\n",
    "            # Specify the file path within the unit's directory\n",
    "            csv_file_path = os.path.join(unit_directory_path, category + '.csv')\n",
    "\n",
    "            # Export the DataFrame to a CSV file\n",
    "            df.to_csv(csv_file_path, index=False)  # Set index=False to exclude the index column\n",
    "\n",
    "            print(f'DataFrame for category \"{category}\" in unit \"{unit}\" has been exported to {csv_file_path}')\n",
    "        else:\n",
    "            print(f\"Error: Unable to fetch the page for category '{category}' in unit '{unit}'.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6103993c2b4544b39e0408ffb719c82c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Checkbox(value=False, description='Check me', indent=False)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "\n",
    "widgets.Dropdown(\n",
    "    options=[('One', 1), ('Two', 2), ('Three', 3)],\n",
    "    value=2,\n",
    "    description='Number:',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
